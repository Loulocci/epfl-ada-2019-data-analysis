{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project : Dunnhumby dataset, Tell me what you buy and I will tell you who you are\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Abstract\" data-toc-modified-id=\"Abstract-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Abstract</a></span></li><li><span><a href=\"#Documentation-of-built-in-functions:\" data-toc-modified-id=\"Documentation-of-built-in-functions:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Documentation of built in functions:</a></span></li><li><span><a href=\"#Data-clean-up-and-overview-of-information-available:\" data-toc-modified-id=\"Data-clean-up-and-overview-of-information-available:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data clean up and overview of information available:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Demographic-data:\" data-toc-modified-id=\"Demographic-data:-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Demographic data:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Demographic-categories:\" data-toc-modified-id=\"Demographic-categories:-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Demographic categories:</a></span></li><li><span><a href=\"#Cleaning-up-household-categories:\" data-toc-modified-id=\"Cleaning-up-household-categories:-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Cleaning up household categories:</a></span></li><li><span><a href=\"#Demographic-distributions:\" data-toc-modified-id=\"Demographic-distributions:-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Demographic distributions:</a></span></li></ul></li><li><span><a href=\"#Product-data:\" data-toc-modified-id=\"Product-data:-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Product data:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quick-overview:\" data-toc-modified-id=\"Quick-overview:-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Quick overview:</a></span></li><li><span><a href=\"#Cleaning-the-data:\" data-toc-modified-id=\"Cleaning-the-data:-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Cleaning the data:</a></span></li></ul></li><li><span><a href=\"#Transaction-data:\" data-toc-modified-id=\"Transaction-data:-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Transaction data:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quick-overview:\" data-toc-modified-id=\"Quick-overview:-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Quick overview:</a></span></li><li><span><a href=\"#Favored-products-by-shoppers:\" data-toc-modified-id=\"Favored-products-by-shoppers:-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Favored products by shoppers:</a></span></li><li><span><a href=\"#Yearly-shopping-occurences:\" data-toc-modified-id=\"Yearly-shopping-occurences:-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Yearly shopping occurences:</a></span></li><li><span><a href=\"#Participation-rate-in-the-study:\" data-toc-modified-id=\"Participation-rate-in-the-study:-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Participation rate in the study:</a></span></li><li><span><a href=\"#Yearly-and-weekly-spending-of-households:\" data-toc-modified-id=\"Yearly-and-weekly-spending-of-households:-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Yearly and weekly spending of households:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Yearly-spending:\" data-toc-modified-id=\"Yearly-spending:-3.3.5.1\"><span class=\"toc-item-num\">3.3.5.1&nbsp;&nbsp;</span>Yearly spending:</a></span></li><li><span><a href=\"#Weekly-spending:\" data-toc-modified-id=\"Weekly-spending:-3.3.5.2\"><span class=\"toc-item-num\">3.3.5.2&nbsp;&nbsp;</span>Weekly spending:</a></span></li></ul></li><li><span><a href=\"#&quot;Loyal&quot;-shoppers:\" data-toc-modified-id=\"&quot;Loyal&quot;-shoppers:-3.3.6\"><span class=\"toc-item-num\">3.3.6&nbsp;&nbsp;</span>\"Loyal\" shoppers:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Average-spending-versus-income:\" data-toc-modified-id=\"Average-spending-versus-income:-3.3.6.1\"><span class=\"toc-item-num\">3.3.6.1&nbsp;&nbsp;</span>Average spending versus income:</a></span></li><li><span><a href=\"#Low-weekly-spending-and-participation-length:\" data-toc-modified-id=\"Low-weekly-spending-and-participation-length:-3.3.6.2\"><span class=\"toc-item-num\">3.3.6.2&nbsp;&nbsp;</span>Low weekly spending and participation length:</a></span></li><li><span><a href=\"#Filtering-&quot;loyal&quot;-shoppers:\" data-toc-modified-id=\"Filtering-&quot;loyal&quot;-shoppers:-3.3.6.3\"><span class=\"toc-item-num\">3.3.6.3&nbsp;&nbsp;</span>Filtering \"loyal\" shoppers:</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Shopping-trends-and-correlations:\" data-toc-modified-id=\"Shopping-trends-and-correlations:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Shopping trends and correlations:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Prepare-data-for-correlation-calulcations:\" data-toc-modified-id=\"Prepare-data-for-correlation-calulcations:-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Prepare data for correlation calulcations:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weekly-grocery-carts:\" data-toc-modified-id=\"Weekly-grocery-carts:-4.0.1.1\"><span class=\"toc-item-num\">4.0.1.1&nbsp;&nbsp;</span>Weekly grocery carts:</a></span></li></ul></li><li><span><a href=\"#Correlation-between-transactions-and-demographic-information:\" data-toc-modified-id=\"Correlation-between-transactions-and-demographic-information:-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Correlation between transactions and demographic information:</a></span></li><li><span><a href=\"#Decision-trees-and-random-forests:\" data-toc-modified-id=\"Decision-trees-and-random-forests:-4.0.3\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;</span>Decision trees and random forests:</a></span></li></ul></li></ul></li><li><span><a href=\"#Next-for-milestone-3:\" data-toc-modified-id=\"Next-for-milestone-3:-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Next for milestone 3:</a></span></li><li><span><a href=\"#Household-carts-clusters:\" data-toc-modified-id=\"Household-carts-clusters:-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Household carts clusters:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Clustering-according-to-exact-products:\" data-toc-modified-id=\"Clustering-according-to-exact-products:-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Clustering according to exact products:</a></span></li><li><span><a href=\"#Go-back-to-labels-and-departments:\" data-toc-modified-id=\"Go-back-to-labels-and-departments:-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Go back to labels and departments:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Labels:\" data-toc-modified-id=\"Labels:-6.0.2.1\"><span class=\"toc-item-num\">6.0.2.1&nbsp;&nbsp;</span>Labels:</a></span></li><li><span><a href=\"#Departments:\" data-toc-modified-id=\"Departments:-6.0.2.2\"><span class=\"toc-item-num\">6.0.2.2&nbsp;&nbsp;</span>Departments:</a></span></li></ul></li><li><span><a href=\"#Overall-label-proportions-with-demographics:\" data-toc-modified-id=\"Overall-label-proportions-with-demographics:-6.0.3\"><span class=\"toc-item-num\">6.0.3&nbsp;&nbsp;</span>Overall label proportions with demographics:</a></span></li></ul></li></ul></li><li><span><a href=\"#Hello-world\" data-toc-modified-id=\"Hello-world-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Hello world</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "We would like to analyse the Dunnhumby dataset. Living in a time and age where every piece of our data is stored and analysed; and being active consumers ourselves, we would like to see what information retail chains can gather and infer about us knowing only our shopping habits. As transactions over two years of several households and their basic demographic profiles are provided, we want to see if there are any links and correlations between specific demographics (e.g. marital status, income, number of children, etc) and purchase patterns. Furthermore, if time permits, we want to see if we can create a model predicting a consumer demographic profile from their shopping. Thus, we would like to see how \"easy\" and how precise it actually is for retailers to infer who their customer is by what they buy and target them with specific marketing. Basically, we want to know how much of a target we actually\n",
    "are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research questions:** \n",
    "- What are the main shopping trends that we can identify in this data?\n",
    "- Can we relate shopping trends to specific demographic parameters?\n",
    "- Can we predict some of these demographic parameters (age, marital statute etc) with knowing the household's habbits?\n",
    "- In the opposite way, can we predict household consumption behaviour with knowing its characteristics?\n",
    "- What accuracy in consumption prediction can the retailer obtain from a simple profile information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#Functions we created:\n",
    "from modules import dem_fx\n",
    "from modules import transaction_fx as trns\n",
    "from modules import plot_functions as plt_fx\n",
    "from modules import ml_functions as ml_fx\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "#Allows reload of modules:\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation of built in functions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we built several functions and put them in the modules folder. All their documentation is available in the doc folder in *Functions Documentation.md*.\n",
    "[Documentation](https://github.com/marvande/epfl-ada-2019-project-bubble582/blob/master/doc/Functions%20Documentation.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clean up and overview of information available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dunnhumby dataset contains household level transactions over two years from a group of 2,500 households who are frequent shoppers at a retailer. It contains all of each households' purchases, not just those from a limited number of categories. For certain households (around 800), demographic information as well as direct marketing contact history are included. We have a look at a few samples from each table: \n",
    "As we said in the description of our project, we are going to concentrate on 3 of the 8 tables :\n",
    "- hh_demographic.csv\n",
    "- transaction_data.csv\n",
    "- product.csv\n",
    "In this first step, we want to load the data, and prepare it for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data: \n",
    "hh_demographic = pd.read_csv(\n",
    "    '../data/dunnhumby_complete_csv/hh_demographic.csv', sep=',')\n",
    "\n",
    "transaction_data = pd.read_csv(\n",
    "    '../data/dunnhumby_complete_csv/transaction_data.csv', sep=',')\n",
    "\n",
    "product = pd.read_csv('../data/dunnhumby_complete_csv/product.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic data: \n",
    "Demographic info for a certain portion of households. The dataset contains only the data for 801 households of 2'500. The rest could not be acquired. The attributes of the dataset are the following: \n",
    " \n",
    "- HOUSEHOLD_KEY : identifies each household, **unique**\n",
    "- AGE_DES: estimated age range\n",
    "- MARITAL_STATUS_CODE: A (Married), B (Single), C (Unknown)\n",
    "- INCOME_DESC : Household income\n",
    "- HOMEOWNER_DESC: Homeowner, renter, etc\n",
    "- HH_COMP_DEC: Household composition\n",
    "- HOUSEHOLD_SIZE_DESC: Size of household up to 5+ \n",
    "- KID_CATEGORY_DESC: Number of children present up to 3+ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at a few samples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_demographic.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic categories:\n",
    "The categories of available information are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Age: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are %d age categories\" %\n",
    "      len(hh_demographic['AGE_DESC'].unique()))\n",
    "print(\"The different categories are:\", hh_demographic['AGE_DESC'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Income: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are %d income categories\" %\n",
    "      len(hh_demographic['INCOME_DESC'].unique()))\n",
    "print(\"The different categories are:\", hh_demographic['INCOME_DESC'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Homeowners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are %d homeowner categories\" %\n",
    "      len(hh_demographic['HOMEOWNER_DESC'].unique()))\n",
    "print(\"The different categories are:\",\n",
    "      hh_demographic['HOMEOWNER_DESC'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Household composition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are %d household composition categories\" %\n",
    "      len(hh_demographic['HH_COMP_DESC'].unique()))\n",
    "print(\"The different categories are:\", hh_demographic['HH_COMP_DESC'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Household size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are %d household size categories\" %\n",
    "      len(hh_demographic['HOUSEHOLD_SIZE_DESC'].unique()))\n",
    "print(\"The different categories are:\",\n",
    "      hh_demographic['HOUSEHOLD_SIZE_DESC'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Kids number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are %d kid number categories\" %\n",
    "      len(hh_demographic['KID_CATEGORY_DESC'].unique()))\n",
    "print(\"The different categories are:\",\n",
    "      hh_demographic['KID_CATEGORY_DESC'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Marital status:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are %d marital status categories\" %\n",
    "      len(hh_demographic['MARITAL_STATUS_CODE'].unique()))\n",
    "print(\"The different categories are:\",\n",
    "      hh_demographic['MARITAL_STATUS_CODE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we change the marital status to a more intuitive one, setting to M the married entries and to S the singles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_demographic.loc[hh_demographic['MARITAL_STATUS_CODE'] == 'A', 'MARITAL_STATUS_CODE'] = 'M'\n",
    "hh_demographic.loc[hh_demographic['MARITAL_STATUS_CODE'] == 'B', 'MARITAL_STATUS_CODE'] = 'S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up household categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three variables related to each other: *household composition*, *household size* and *number of kids*. Thus, we need to see if the information between them makes sense (e.g. if the number of kids corresponds to the correct household size, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first going to look what kind of values are available for each possible household composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hh_composition in hh_demographic['HH_COMP_DESC'].unique():\n",
    "    if hh_composition == \"2 Adults No Kids\" or hh_composition == \"2 Adults Kids\":\n",
    "        continue\n",
    "    print(\"Looking at the household composition:\", hh_composition)\n",
    "    hh_demographic_current_composition = hh_demographic[\n",
    "        hh_demographic['HH_COMP_DESC'] == hh_composition]\n",
    "    print(\"Household size unique information:\",\n",
    "          hh_demographic_current_composition['HOUSEHOLD_SIZE_DESC'].unique())\n",
    "    print(\"Kids number unique information:\",\n",
    "          hh_demographic_current_composition['KID_CATEGORY_DESC'].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already see some inconsistencies. For example, a household of a single female can hardly be a household of size two. Except if she has kids but then she should be in a household of \"1 Adult Kids\". The same applies for the other types. This needs to be corrected before we proceed any further. For this we need to know which entries are wrong so we take a closer look at each household composition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Closer look at \"1 Adult Kids\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_composition = \"1 Adult Kids\"\n",
    "hh_demographic_1adultkids = hh_demographic[hh_demographic['HH_COMP_DESC'] ==\n",
    "                                           hh_composition]\n",
    "\n",
    "for household_size in hh_demographic_1adultkids['HOUSEHOLD_SIZE_DESC'].unique():\n",
    "    hh_demographic_1adultkids_size = hh_demographic_1adultkids[\n",
    "        hh_demographic_1adultkids['HOUSEHOLD_SIZE_DESC'] == household_size]\n",
    "\n",
    "    print(f\"For household of size {household_size}, with 1 adult, there are\",\n",
    "          hh_demographic_1adultkids_size['KID_CATEGORY_DESC'].unique(),\n",
    "          \"kid categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are inconsistencies when the household size is of 3 or 4 units. Let's explore these further:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Household of 3 units:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_demographic_1adultkids[\n",
    "    (hh_demographic_1adultkids['HOUSEHOLD_SIZE_DESC'] == '3')\n",
    "    & (hh_demographic_1adultkids['KID_CATEGORY_DESC'] == '1')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that there are entries where the composition is *1 Adult Kids*, the household size is 3 and the number of kids is 1. This means one of the following: there is one more person living in the house, the household size is wrong or the composition is wrong.<br>\n",
    "However, we  notice that all of these entries have a marital status M, which stands for married. We can assume then that the composition is wrong and there is an actual couple living in the house, with 1 kid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Household of 4 units:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_demographic_1adultkids[\n",
    "    (hh_demographic_1adultkids['HOUSEHOLD_SIZE_DESC'] == '4')\n",
    "    & (hh_demographic_1adultkids['KID_CATEGORY_DESC'] == '2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For household size of 4, the same as for 3 stands. Again the household composition is probably wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Closer look at \"Single Male/Female\": "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the household should not be bigger than 1. So we look at the marital status of households of size 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_composition = \"Single Female\"\n",
    "\n",
    "hh_demographic_singlefemale_size2 = hh_demographic[\n",
    "    hh_demographic['HH_COMP_DESC'] == hh_composition]\n",
    "\n",
    "hh_demographic_singlefemale_size2 = hh_demographic_singlefemale_size2[\n",
    "    hh_demographic_singlefemale_size2['HOUSEHOLD_SIZE_DESC'] == '2']\n",
    "\n",
    "print(hh_demographic_singlefemale_size2['MARITAL_STATUS_CODE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_composition = \"Single Male\"\n",
    "\n",
    "hh_demographic_singlemale_size2 = hh_demographic[hh_demographic['HH_COMP_DESC']\n",
    "                                                 == hh_composition]\n",
    "\n",
    "hh_demographic_singlemale_size2 = hh_demographic_singlemale_size2[\n",
    "    hh_demographic_singlemale_size2['HOUSEHOLD_SIZE_DESC'] == '2']\n",
    "\n",
    "print(hh_demographic_singlemale_size2['MARITAL_STATUS_CODE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again it looks like they are married and not single and that the household composition is wrong.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Coherence between marital status, number of kids and  household size:\n",
    "We now want to check if the marital status, the number of kids and the household size are always coherent with each others. If it is, then we can assume that the household composition information is sometimes wrong. Hence, we can correct this parameter or just discard it, since it does not carry more information with respect to the other three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for marital_status in np.sort(hh_demographic['MARITAL_STATUS_CODE'].unique()):\n",
    "    print(\"Marital status:\", marital_status)\n",
    "    \n",
    "    hh_demographic_current_marital = hh_demographic[\n",
    "        hh_demographic['MARITAL_STATUS_CODE'] == marital_status]\n",
    "    \n",
    "    print(\n",
    "        hh_demographic_current_marital.groupby(\n",
    "            ['HH_COMP_DESC', 'HOUSEHOLD_SIZE_DESC',\n",
    "             'KID_CATEGORY_DESC']).size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that: \n",
    "- the marital status is always coherent with the household size and the number of kids. Combined with the findings above, we can say that in these cases where the household composition is wrong, we will just discard that information.\n",
    "- we have some incongruities in the household size / number of kids when the marital status is Single, so we discard these entries.\n",
    "- if the marital status is *Unknown*, we fall back on the household size / number of children information and we give the corresponding marital status, when it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Actually removing the inconsistencies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_demographic_fxd = hh_demographic.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the entries marked as Single with inconsistencies in the household size / number of kids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropindex = hh_demographic_fxd.index[\n",
    "    (hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'S')\n",
    "    & (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '2') &\n",
    "    (hh_demographic_fxd['KID_CATEGORY_DESC'] == 'None/Unknown')].tolist()\n",
    "\n",
    "dropindex += hh_demographic_fxd.index[\n",
    "    (hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'S')\n",
    "    & (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '3') &\n",
    "    (hh_demographic_fxd['KID_CATEGORY_DESC'] == '1')].tolist()\n",
    "\n",
    "dropindex += hh_demographic_fxd.index[\n",
    "    (hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'S')\n",
    "    & (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '4') &\n",
    "    (hh_demographic_fxd['KID_CATEGORY_DESC'] == '2')].tolist()\n",
    "\n",
    "print(dropindex)\n",
    "print(len(dropindex), \"entries dropped.\")\n",
    "\n",
    "hh_demographic_fxd.drop(dropindex, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning the correct marital status to the entries marked as Unknown, when the household size and the number of children are coherent with each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_demographic_fxd.loc[(hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'U') &\n",
    "                       (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '3') &\n",
    "                       (hh_demographic_fxd['KID_CATEGORY_DESC'] == '1'\n",
    "                        ), 'MARITAL_STATUS_CODE'] = 'M'\n",
    "\n",
    "hh_demographic_fxd.loc[(hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'U') &\n",
    "                       (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '4') &\n",
    "                       (hh_demographic_fxd['KID_CATEGORY_DESC'] == '2'\n",
    "                        ), 'MARITAL_STATUS_CODE'] = 'M'\n",
    "\n",
    "hh_demographic_fxd.loc[(hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'U') &\n",
    "                       (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '2') &\n",
    "                       (hh_demographic_fxd['KID_CATEGORY_DESC'] ==\n",
    "                        'None/Unknown'), 'MARITAL_STATUS_CODE'] = 'M'\n",
    "\n",
    "hh_demographic_fxd.loc[(hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'U') &\n",
    "                       (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '2') &\n",
    "                       (hh_demographic_fxd['KID_CATEGORY_DESC'] == '1'\n",
    "                        ), 'MARITAL_STATUS_CODE'] = 'S'\n",
    "\n",
    "hh_demographic_fxd.loc[(hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'U') &\n",
    "                       (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '3') &\n",
    "                       (hh_demographic_fxd['KID_CATEGORY_DESC'] == '2'\n",
    "                        ), 'MARITAL_STATUS_CODE'] = 'S'\n",
    "\n",
    "hh_demographic_fxd.loc[(hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'U') &\n",
    "                       (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '4') &\n",
    "                       (hh_demographic_fxd['KID_CATEGORY_DESC'] == '3+'\n",
    "                        ), 'MARITAL_STATUS_CODE'] = 'S'\n",
    "\n",
    "hh_demographic_fxd.loc[(hh_demographic_fxd['MARITAL_STATUS_CODE'] == 'U') &\n",
    "                       (hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] == '1') &\n",
    "                       (hh_demographic_fxd['KID_CATEGORY_DESC'] ==\n",
    "                        'None/Unknown'), 'MARITAL_STATUS_CODE'] = 'S'\n",
    "\n",
    "hh_demographic_fxd = hh_demographic_fxd[\n",
    "    hh_demographic_fxd['MARITAL_STATUS_CODE'] != 'U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for marital_status in np.sort(hh_demographic_fxd['MARITAL_STATUS_CODE'].unique()):\n",
    "    print(\"Marital status:\", marital_status)\n",
    "    \n",
    "    hh_demographic_current_marital = hh_demographic_fxd[\n",
    "        hh_demographic_fxd['MARITAL_STATUS_CODE'] == marital_status]\n",
    "    \n",
    "    print(\n",
    "        hh_demographic_current_marital.groupby(\n",
    "            ['HH_COMP_DESC', 'HOUSEHOLD_SIZE_DESC',\n",
    "             'KID_CATEGORY_DESC']).size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of entries goes from {len(hh_demographic.count(axis='columns')):n} to {len(hh_demographic_fxd.count(axis='columns')):n}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_demographic_fxd.drop(['HH_COMP_DESC'], axis=1, inplace=True)\n",
    "\n",
    "hh_demographic_fxd.loc[hh_demographic_fxd['KID_CATEGORY_DESC'] ==\n",
    "                       'None/Unknown', 'KID_CATEGORY_DESC'] = '0'\n",
    "\n",
    "hh_demographic_fxd.rename(columns={'KID_CATEGORY_DESC': 'KIDS_DESC'},\n",
    "                          inplace=True)\n",
    "\n",
    "hh_demographic_fxd['KIDS_DESC'] = hh_demographic_fxd['KIDS_DESC'].fillna('0')\n",
    "\n",
    "hh_demographic_fxd.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the fixed dataframe:\n",
    "if not os.path.exists(\"saved_structures\"):\n",
    "    os.makedirs(\"saved_structures\")\n",
    "hh_demographic_fxd.to_csv(\"saved_structures/hh_demographic_fix_hhcomp.csv\",\n",
    "                          sep='\\t',\n",
    "                          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conclusion: \n",
    "42 entries were discarded because the marital status Single did not match with the household size / number of children or because, in the Unknown marital status, the household size and the number of children did not carry enough information to conclude something on the marital status. We carry on the rest of our data journey with this cleaned demographic dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed inconsistencies in household composition, we want to have a look at how much data is present for each category. For now, the categories in this data frame are not arranged in a meaningful way, meaning that if we were making some plots now, we would not have the age categories ranged in ascending or descending order for example. \n",
    "Thus, we first want to arrange them, before making some exploratory plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going make the columns in the dataframe with categories of the ordered categorical type:\n",
    "hh_demographic_fxd['AGE_DESC'] = dem_fx.order_hh_dem(\n",
    "    hh_demographic_fxd, 'AGE_DESC',\n",
    "    ['19-24', '25-34', '35-44', '45-54', '55-64', '65+'])\n",
    "\n",
    "hh_demographic_fxd['INCOME_DESC'] = dem_fx.order_hh_dem(\n",
    "    hh_demographic_fxd, 'INCOME_DESC', \n",
    "    ['Under 15K', '15-24K', '25-34K', '35-49K', '50-74K', '75-99K',\n",
    "        '100-124K', '125-149K', '150-174K', '175-199K', '200-249K', '250K+'])\n",
    "\n",
    "hh_demographic_fxd['HOMEOWNER_DESC'] = dem_fx.order_hh_dem(\n",
    "    hh_demographic_fxd, 'HOMEOWNER_DESC',\n",
    "    ['Unknown', 'Probable Renter', 'Renter', 'Probable Owner', 'Homeowner'])\n",
    "\n",
    "hh_demographic_fxd['HOUSEHOLD_SIZE_DESC'] = dem_fx.order_hh_dem(\n",
    "    hh_demographic_fxd, 'HOUSEHOLD_SIZE_DESC', ['1', '2', '3', '4', '5+'])\n",
    "\n",
    "hh_demographic_fxd['KIDS_DESC'] = dem_fx.order_hh_dem(\n",
    "    hh_demographic_fxd, 'KIDS_DESC', ['0', '1', '2', '3+'])\n",
    "\n",
    "hh_demographic_fxd['MARITAL_STATUS_CODE'] = dem_fx.order_hh_dem(\n",
    "    hh_demographic_fxd, 'MARITAL_STATUS_CODE', ['M', 'S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the categories in this data frame are ranged in a meaningful way, let's make some simple plots to have an idea of the characteristics of the population which we study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_fx.plot_demographics(hh_demographic_fxd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on graph**: We see that we don't have uniformly distributed demographic data. We know now that: \n",
    "- Age: most of our households are between 35-54 years old. We note though that we are not sure who's age this represent. Most of the time, it's the age of the male of the house when there are two adults. For single households it would be the adult's age. This just needs to be remembered. We do not have an idea of the age of everyone in our data. \n",
    "- Marital status: most households are married though we also have a lot of singles.\n",
    "- Household size: majority of households of 2 or 1 people/person. \n",
    "- Income: most people between 35-75 K (the average American income is 50 K so this seems all right)\n",
    "- Homeowner: most households are homeowners, still a lot of unknowns though. \n",
    "- Kids: Most households do not have kids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product data: \n",
    "Information on each product sold such as type of product, national or private label and a brand identifier. The attributes of the dataset are the following: \n",
    "- PRODUCT_ID: identifies product, **unique**\n",
    "- DEPARMENT: groups similar products together\n",
    "- COMMODITY_DESC: groups similar products together at a lower level\n",
    "- SUB_COMMODITY_DESC: groups similar products together at the lowest level\n",
    "- MANUFACTURER: code that links products with the same manufacturer together \n",
    "- BRAND: indicates private or national label brand\n",
    "- CURR_SIZE_OF_PRODUCT: indicates package size (not available for all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick overview: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Number of products: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are \" + f\"{product.count()['PRODUCT_ID']:,d}\" +\n",
    "      \" different products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Product departments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there are \" + f\"{len(product['DEPARTMENT'].unique()) :,d}\" +\n",
    "      \" department categories.\")\n",
    "print(\"The different categories are:\", product['DEPARTMENT'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Representation of products in transactions: \n",
    "There are 92 353 products. As for the households, we can investigate whether all the products are represented in the *transaction_data* table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \" + f\"{len(transaction_data['PRODUCT_ID'].unique()):,d}\" +\n",
    "      \" products in the transactions table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 92 339 products represented in the *transaction_data* table, meaning that only **14** are not represented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data:\n",
    "Right now, we have a lot of departments that seem similar and we would like to get a better idea of what is actually in the dataframe. For this, in the *Products* notebook, we categorized all products into 17 labels. It is not included in this notebook as it takes a lot of running time. If you don't want to run the products notebook, the csv file is available in the google drive link given in the README of code. We are just going to load the resulting dataframe with clean labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset with updated product labels:\n",
    "#labelled_prod = pd.read_csv('saved_structures/updated_prod3.csv', sep='\\t')\n",
    "labelled_prod = pd.read_csv('saved_structures/updated_prod_precise.csv', sep='\\t')\n",
    "#Drop columns we are not interested in:\n",
    "labelled_prod = labelled_prod.set_index('PRODUCT_ID').drop(\n",
    "    ['MANUFACTURER', 'BRAND', 'CURR_SIZE_OF_PRODUCT'], axis=1)\n",
    "labelled_prod.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_fx.cat_count_plot(labelled_prod['LABEL'],\n",
    "                      'Label distribution in our product data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that we have labels for each product that categorizes them into different categories. We also see that most of our products fall under processed foods and households. Next, with transaction data, we are going to look at what kind of products households buy most frequently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset of all products purchased by households during the study. Each line in the table is what could essentially be found in a store reciept. The attributes of the dataset are the following: \n",
    "\n",
    "- HOUSEHOLD_KEY: identifies each household, **unique**\n",
    "- BASKET_ID: identifies a purchase occasion\n",
    "- DAY: day when transaction occured\n",
    "- PRODUCT_ID: identifies each product\n",
    "- QUANTITY: Number of products purchased during trip\n",
    "- SALES_VALUE: Amount of dollars retailer recieves from sale\n",
    "- STORE_ID: identifies store\n",
    "- COUPON_MATCH_DISC: discount applied du to retailer's match of manufacturer coupon\n",
    "- COUPON_DISC: discount applied due to manufacturer coupon\n",
    "- RETAIL_DISC: discount applied due to retailer's loyalty card program\n",
    "- TRANS_TIME: time of day when transaction occured\n",
    "- WEEK_NO: week of the transaction. Ranges from 1-102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the coupons columns as we're not interessed in marketing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_initial_clean = transaction_data.drop(\n",
    "    ['COUPON_DISC', 'COUPON_MATCH_DISC', 'RETAIL_DISC'], axis=1)\n",
    "\n",
    "trans_initial_clean_hous_ind = trans_initial_clean[\n",
    "    'household_key'].sort_values().unique()\n",
    "\n",
    "trans_initial_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick overview: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Number of total transactions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there were \" +\n",
    "      f\"{transaction_data.count()['household_key']:,d}\" +\n",
    "      \" transactions during the two years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Number of total purchase occasions: \n",
    "Attention: here transactions are not what we usually think of. It's like on a receipt, so the number of total unique purchases is not the number of transactions but the total number of unique *basket_id*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there were \" +\n",
    "      f\"{len(transaction_data['BASKET_ID'].unique()):,d}\" +\n",
    "      \" purchase occasions during the two years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Number of households in transaction dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In total there were \" +\n",
    "      f\"{len(transaction_data['household_key'].unique()):,d}\" +\n",
    "      \" households represented during the two years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Favored products by shoppers:\n",
    "Combine the information from transactions with the new labeled products dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What departments and labels were used in our transaction data ?\n",
    "We assume that most transactions fall in the department of grocery and drug GM (general merchandise). Let's see if we're right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_trans = trans_initial_clean.join(other=labelled_prod,\n",
    "                                          on='PRODUCT_ID').dropna()\n",
    "plt_fx.cat_count_plot(\n",
    "    labelled_trans['DEPARTMENT'],\n",
    "    'Number of occurences per department in our transaction data')\n",
    "\n",
    "plt_fx.cat_count_plot(\n",
    "    labelled_trans['LABEL'],\n",
    "    'Number of occurences per label in our transaction data')\n",
    "plt.savefig(\"products_trans_label.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on graph**: \n",
    "\n",
    "Like we predicted, most of the transactions that occurred during the two year study were *Groceries*, *Drug and General Merchandise* or *Produce*. Here General Merchandise is a broad catchall term for pretty much everything that's not groceries – from hearing aid batteries to major appliances. Furthermore, the 6 labels of products that were used most are: \n",
    "- Processed foods\n",
    "- Produce\n",
    "- Dairy\n",
    "- Meat & seafood\n",
    "- Beverages (does not include alcoholic beverages)\n",
    "- Households\n",
    "\n",
    "Here, we need to point out the \"Processed foods\" is a label for everything food related that is not fresh produce (vegetables, dairy, meat, etc) in groceries like Honey, Cookies, Muffins, ready made dinners, dips, etc. We can see that in the next table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_prod[labelled_prod['LABEL'] == 'PROCESSED FOODS'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to look at how much households spend on groceries we're going to keep only transactions that are in the department of *Groceries*, *Drug and General Merchandise*, *Produce* *Meat*, *Deli* and *Packaged meat*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter transaction data to contain only the major departments:\n",
    "list_of_departments = ['GROCERY', 'DRUG GM', 'PRODUCE', 'MEAT', 'MEAT-PCKGD', 'DELI']\n",
    "\n",
    "trans_clean = labelled_trans[labelled_trans['DEPARTMENT'].apply(\n",
    "    lambda x: x in list_of_departments)]\n",
    "\n",
    "trans_clean_hous_ind = trans_clean['household_key'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly shopping occurences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the assumption here that a year is 51 weeks. We could have also looked at the days but weeks is easier as there are 102 weeks in the dataset. We are first going to look at the number of total purchase per year to get an idea of shopping frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Households per year:\n",
    "trans_clean_year_1 = trans_clean[trans_clean['WEEK_NO'].apply(\n",
    "    lambda x: x <= 51)]\n",
    "trans_clean_year_2 = trans_clean[trans_clean['WEEK_NO'].apply(\n",
    "    lambda x: x > 51)]\n",
    "\n",
    "#Get the household indices for each year:\n",
    "trans_clean_hous_ind_1 = trans_clean_year_1['household_key'].sort_values().unique()\n",
    "trans_clean_hous_ind_2 = trans_clean_year_2['household_key'].sort_values().unique()\n",
    "\n",
    "purch_per_household_year1 = trns.trans_per_year(\n",
    "    trans_clean_year_1, trans_clean_hous_ind_1)\n",
    "purch_per_household_year2 = trns.trans_per_year(\n",
    "    trans_clean_year_2, trans_clean_hous_ind_2)\n",
    "\n",
    "print('Mean of total purchases in the first year per household is %d purchases per year'\n",
    "    % purch_per_household_year1['total purchase per year'].mean())\n",
    "print('Median of total purchases in the first year per household is %d purchases per year'\n",
    "    % purch_per_household_year1['total purchase per year'].median())\n",
    "print('\\n')\n",
    "print('Mean of total purchases in the second year per household is %d purchases per year'\n",
    "    % purch_per_household_year2['total purchase per year'].mean())\n",
    "print('Median of total purchases in the second year per household is %d purchases per year'\n",
    "    % purch_per_household_year2['total purchase per year'].median())\n",
    "\n",
    "#Plot distribution if we want to look at them:\n",
    "titles = [\n",
    "    'Distribution of total purchases over the first year per household',\n",
    "    'Distribution of total purchaes over the second year per household']\n",
    "xlabels = [\n",
    "    'Number of total purchases in the first year',\n",
    "    'Number of total purchases in the second year']\n",
    "\n",
    "plt_fx.double_plot_box_dist(\n",
    "    purch_per_household_year1['total purchase per year'],\n",
    "    purch_per_household_year2['total purchase per year'], titles, xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the graph**: \n",
    "\n",
    "We learned that, although the mean of households goes shopping approximately once per week, half of the households goes shopping less than once per week (through the median). Usually, if households did all their shopping at this retailer, we would assume that they should buy groceries at least every week in average. So we can guess already here that those households probably also do their shopping elsewhere than just this particular retailer. We hoped that through transaction data we could have an idea of weekly food habits of households. Nevertheless this would not work if we only have partial information about what they buy in total per week (e.g. if they also buy it from somewhere else). We would only have information about habits at this local retailer. To confirm this we need to look at how much households spend and their specific participation length in the study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participation rate in the study:\n",
    "For how long did households participate in the study? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if all households are present both years:\n",
    "missing_households_year1 = set(list(range(1, 2501))).difference(\n",
    "    set(trans_clean_year_1['household_key'].unique()))\n",
    "\n",
    "missing_households_year2 = set(list(range(1, 2501))).difference(\n",
    "    set(trans_clean_year_2['household_key'].unique()))\n",
    "\n",
    "print(\n",
    "    \"The following households are not represented in the first year transaction data: \",\n",
    "    missing_households_year1)\n",
    "print(\n",
    "    \"The following households are not represented in the second year transaction data: \",\n",
    "    missing_households_year2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some households were not present in both years of the study. To study this further we look at the study participation length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframes of spending per household (how much they paid for each purchase)\n",
    "#for each year and in total over both years:\n",
    "grouped_trans_spent_total = trns.spending_per_household_per_trans(\n",
    "    trans_clean)\n",
    "grouped_trans_spent_year1 = trns.spending_per_household_per_trans(\n",
    "    trans_clean_year_1)\n",
    "grouped_trans_spent_year2 = trns.spending_per_household_per_trans(\n",
    "    trans_clean_year_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pandas dataframe with participation length per household key:\n",
    "participation_per_hh = pd.DataFrame(\n",
    "    index=trans_clean_hous_ind,\n",
    "    data={\n",
    "        'participation_length': [\n",
    "            len(grouped_trans_spent_total.loc[i]['WEEK_NO'].unique())\n",
    "            for i in trans_clean_hous_ind\n",
    "        ]\n",
    "    })\n",
    "\n",
    "#Plot participation:\n",
    "plt_fx.plot_box_dist(\n",
    "    participation_per_hh['participation_length'],\n",
    "    'Distribution of participation duration in the study',\n",
    "    'Lenght of participation [weeks]')\n",
    "plt.savefig(\"participation.png\")\n",
    "\n",
    "print(\n",
    "    f\"There were {len(participation_per_hh[participation_per_hh['participation_length'] >= 100]):n} households that participated during the whole two years.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: \n",
    "\n",
    "We realize that some households were not present in the first year and some other dropped out of the study in the second year. But actually, we see that most households only participated between 20 and 80 weeks. Only 3 households did over 100 weeks. For now, we are going to keep all households and not filter them on participation length because we don't know if participation is correlated with shopping trends. Also we already don't have that much household data (only 750 over 2500) and we don't want to loose any more. We just need to keep this in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly and weekly spending of households:\n",
    "To follow up on our question about whether households do most of their shopping at this retailer, we need to look at how much they spent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yearly spending:\n",
    "For this we created three dataframes, two of total spending for each year and one with the mean yearly spending. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframes of how much families spent per year (for year 1, 2 and for the mean of both years):\n",
    "budget_first_year = trns.bud_total_per_year(grouped_trans_spent_year1,\n",
    "                                            trans_clean_hous_ind_1)\n",
    "\n",
    "budget_second_year = trns.bud_total_per_year(grouped_trans_spent_year2,\n",
    "                                             trans_clean_hous_ind_2)\n",
    "\n",
    "#Dataframes of mean yearly spending:\n",
    "mean_yearly_spend = trns.mean_yearly_spending(budget_first_year,\n",
    "                                              budget_second_year)\n",
    "\n",
    "mean1 = budget_first_year['yearly spending'].mean()\n",
    "median1 = budget_first_year['yearly spending'].median()\n",
    "\n",
    "mean2 = budget_second_year['yearly spending'].mean()\n",
    "median2 = budget_second_year['yearly spending'].median()\n",
    "\n",
    "print('Mean of budget in the first year per household is ' +\n",
    "      f\"{round(mean1, 2):,}\" + ' dollars per year')\n",
    "print('Median of budget in the first year per household is ' +\n",
    "      f\"{round(median1, 2):,}\" + ' dollars per year')\n",
    "print('\\n')\n",
    "print('Mean of budget in the second year per household is ' +\n",
    "      f\"{round(mean2, 2):,}\" + ' dollars per year')\n",
    "print('Median of budget in the second year per household is ' +\n",
    "      f\"{round(median2, 2):,}\" + ' dollars per year')\n",
    "\n",
    "titles = [\n",
    "    'Amount spent per household in the first year',\n",
    "    'Amount spent per household in the second year'\n",
    "]\n",
    "xlabels = ['Amount spent per year', 'Amount spent per year']\n",
    "\n",
    "plt_fx.double_plot_box_dist(budget_first_year['yearly spending'],\n",
    "                            budget_second_year['yearly spending'], titles,\n",
    "                            xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment:*\n",
    "\n",
    "Most families during the two years spent between ~400 dollars to 2000 dollars per year. However, we see that there are outliers households that spent over 4000 dollars. It would be interesting to see if those are families with higher income or something else that demographically differentiates them from the rest. \n",
    "\n",
    "Furthermore, as 400 dollars per year does not make a lot of sense in term of regular spending at a retailer, this further confirms our suspicions of partial shopping at this retailer. To follow up on this, We need to have an idea of how much households spent per week in average as it is easier to relate to than yearly spending. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weekly spending:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define average weekly spending as the total amount of money spent per household divided by the number of weeks they participated. We plot the distribution of mean yearly and weekly spending. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_budget_week = trns.df_weekly_spending(trans_clean)\n",
    "\n",
    "mean = [\n",
    "    mean_yearly_spend['mean yearly spending'].mean(),\n",
    "    mean_budget_week['mean weekly spending'].mean()\n",
    "]\n",
    "\n",
    "median = [\n",
    "    mean_yearly_spend['mean yearly spending'].median(),\n",
    "    mean_budget_week['mean weekly spending'].median()\n",
    "]\n",
    "\n",
    "print('Mean of weekly budget per family ' + f\"{round(mean[1], 2):,}\" +\n",
    "      ' dollars per week')\n",
    "print('Median of weekly budget per family is ' + f\"{round(median[1], 2):,}\" +\n",
    "      ' dollars per week')\n",
    "\n",
    "plt_fx.double_plot_box_dist(\n",
    "    mean_yearly_spend['mean yearly spending'],\n",
    "    mean_budget_week['mean weekly spending'],\n",
    "    ['Mean yearly amount per household', 'Weekly amount spent per household'],\n",
    "    ['Yearly spending [$]', 'Weekly spending [$]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There were {len(mean_budget_week[mean_budget_week['mean weekly spending'] < 50]):n} households with a budget under 50 dollars per week.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There were {len(mean_budget_week[mean_budget_week['mean weekly spending'] > 120]):n} households with a budget over 120 dollars per week.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** \n",
    "\n",
    "We notice that most households spend between \\\\$50-100 per week on purchases. But still a lot of households spent less than \\\\$50 per week. That seems too low for a weekly budget of full groceries as the weekly amount set by \"[Business insider]( https://www.businessinsider.com/what-americans-spend-on-groceries-every-month-2019-4?r=US&IR=T#22-dallas-fort-worth-1)\" for the US is around \\\\$70. This again confirms that households do only part of their shopping at this retailer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Loyal\" shoppers: \n",
    "By loyal we mean that they might do most of their shopping at this retailer. In a perfect world, to get information about food habits from these household transactions, we would need all households to do most of their grocery shopping at this retailer. To filter them out from our data, we are going to look at average spending versus income and participation length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average spending versus income:\n",
    "To find out how many households are approximate \"full shoppers\" at this retailer we look at what households spent according to their income. For this we need to combine our transaction data with some demographic information. Note: we only have information about around 750 households from the 2500 present in the transaction data. This is going to considerably reduce the information we can pull from this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_df = hh_demographic_fxd\n",
    "demographic_df = demographic_df.set_index('household_key')\n",
    "demographic_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new demographic data frame where a column with *mean yearly budget* and *mean weekly budget* are added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with weekly and yearly spending:\n",
    "#The yearly spending is the mean over the two years\n",
    "hh_spending = mean_budget_week.join(mean_yearly_spend).dropna()\n",
    "\n",
    "hh_spending = hh_spending.join(demographic_df).dropna()\n",
    "\n",
    "print(\n",
    "    f\"Note: There are {len(hh_spending.index):n} households in this new dataframe\"\n",
    ")\n",
    "\n",
    "#Save to csv:\n",
    "if not os.path.exists(\"saved_structures\"):\n",
    "    os.makedirs(\"saved_structures\")\n",
    "hh_spending.to_csv(\"saved_structures/hh_spending.csv\", sep='\\t', index=False)\n",
    "\n",
    "hh_spending.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the weekly and yearly spending versus income for each income category: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Yearly spending versus income', 'Weekly spending versus income']\n",
    "\n",
    "print(\n",
    "    f\"The mean of yearly spending per household is {hh_spending['mean yearly spending'].mean():n}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The mean of weekly spending per household is {hh_spending['mean weekly spending'].mean():n}.\"\n",
    ")\n",
    "\n",
    "plt_fx.double_categorical_scatter(hh_spending['INCOME_DESC'],\n",
    "                                  hh_spending['mean yearly spending'],\n",
    "                                  hh_spending['INCOME_DESC'],\n",
    "                                  hh_spending['mean weekly spending'],\n",
    "                                  titles,\n",
    "                                  add_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** \n",
    "\n",
    "We note that weekly spending and income does not augment linearly. We might have thought that a higher income creates higher spending habits but from what we see here it does not seem like it. For now it looks like the average spending at this retailer is not regular per income levels. Again this hints that the part of households that do most of their shopping at this retailer or only part of it varies greatly among all income levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Low weekly spending and participation length:\n",
    "\n",
    "Because of the look of the plots above, we look at the relation between weekly spending and participation length. For example if households with low spending did not participate for a long time, this might indicate that those households only bought a few times and not a lot and that their data cannot be considered because they might not give any information about shopping habits. Thus we plot participation length for households that spent less than \\$25 per week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe with participation length added to demographic data:\n",
    "participation_hh_dem = participation_per_hh.join(hh_spending).dropna()\n",
    "\n",
    "#Look at households that spend under 25 dollars per week:\n",
    "under25 = participation_hh_dem[\n",
    "    participation_hh_dem['mean weekly spending'] <= 25]\n",
    "\n",
    "#Plot participation versus income:\n",
    "titles = [\n",
    "    'Income versus mean weekly spending for households with low spending',\n",
    "    'Income versus study participation length for households with low spending'\n",
    "]\n",
    "\n",
    "plt_fx.double_categorical_scatter(under25['INCOME_DESC'],\n",
    "                                  under25['mean weekly spending'],\n",
    "                                  under25['INCOME_DESC'],\n",
    "                                  under25['participation_length'], titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on graph:**\n",
    "\n",
    "We note that households with low weekly spending still participated for at least around 40 weeks so we cannot say that those households just participated once and did it \"badly\" and that their data cannot be taken into account when looking for shopping trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering \"loyal\" shoppers: \n",
    "For this wee need information about usual grocery spending habits, we use statistics available online: <br>\n",
    "\n",
    "According to the [Bureau of Labor statistics:](https://www.thestreet.com/personal-finance/average-cost-of-food-14845479)\n",
    "- Food spending: more than \\\\$7'7000 per year on groceries and more .\n",
    "    - one member household : \\\\$4,425\n",
    "    - Two members : \\\\$7,865\n",
    "    - Four members : \\\\$10,995\n",
    "- Income levels: attention groceries do not include eating-out ! \n",
    "    - lowest 20%: Mean Income of \\\\$11,394 -> \\\\$2,582 on groceries per year. **On what kind of produce do they spend it ?**\n",
    "    - second 20%: Mean income \\\\$29,821 -> \\\\$3,622 on groceries per year. \n",
    "    - third 20%: Mean income \\\\$52,431 -> \\\\$4,038 on groceries per year. \n",
    "    - fourth 20%: Mean Income of \\\\$86,363 -> \\\\$4,893 on groceries\n",
    "    - top 20%: Mean Income of \\\\$188,103 -> \\\\$6,677 on groceries\n",
    "    \n",
    "<br>\n",
    "So we know that in our data, every household (in the lower income tier) should have spent at least around \\$2500 dollars per year. Households that spent less than that did not participate in the study fully and bought groceries elsewhere. We thus make the assumption that to be a \"loyal\" shopper, they should spend at least \\$2500 per year. We set the same bound for everyone for the moment just in case we have some really frugal households in higher income categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove in the dataset:\n",
    "filtered_income = participation_hh_dem[\n",
    "    participation_hh_dem['mean yearly spending'] >= 2500]\n",
    "\n",
    "print(\n",
    "    f\"We lost {len(participation_hh_dem.index) - len(filtered_income.index):d} households by filtering.\"\n",
    ")\n",
    "print(\n",
    "    f\"We have {len(filtered_income.index):d} households left after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles1 = [\n",
    "    'Income versus mean yearly spending for households with spending > 2500',\n",
    "    'Income versus study participation length for households with spending > 2500'\n",
    "]\n",
    "\n",
    "plt_fx.double_categorical_scatter(filtered_income['INCOME_DESC'],\n",
    "                                  filtered_income['mean yearly spending'],\n",
    "                                  filtered_income['INCOME_DESC'],\n",
    "                                  filtered_income['participation_length'],\n",
    "                                  titles1)\n",
    "\n",
    "titles2 = [\n",
    "    'Age versus mean yearly spending for households with spending >2500',\n",
    "    'Household size versus yearly spending for households with spending > 2500'\n",
    "]\n",
    "\n",
    "plt_fx.double_categorical_scatter(filtered_income['AGE_DESC'],\n",
    "                                  filtered_income['mean yearly spending'],\n",
    "                                  filtered_income['HOUSEHOLD_SIZE_DESC'],\n",
    "                                  filtered_income['mean yearly spending'],\n",
    "                                  titles2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "We don't have that many \"loyal\" shoppers (only 286 on 800) but at least we can see that most of those households participated for more than a year. Still the real number of \"loyal\" households is probably lower as there are households of 5+ that spent less than 3K/year which seems unreasonable if that is their only shopping. <br> \n",
    "\n",
    "**Note**: We see that there are some outliers, like households with income under 15K that spent around 8K per year on groceries. That seems like a miss-classification (probably wrong income).\n",
    "\n",
    "So in conclusion for this part, we learned that there are two categories of shoppers at this retailer; those that do most of their grocery shopping there every week and those who only do part of it. As the number of households that do all their shopping at this retailer is really low and hard to determine, we will not be able to find out any information about total food habits of households. Nevertheless, we can try to find out if there is information to be found about shopping trends at this retailer (e.g. do 65+ households tend to buy more produce here than 25-34)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shopping trends and correlations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for correlation calulcations: \n",
    "\n",
    "We have two interesting dataframes:\n",
    "- demographic data for each household and their weekly spending, yearly spending and participation in the study \n",
    "- transactions of all households and the labels of each product.\n",
    "\n",
    "We are going to combine them to get information about the quantities of products households bought weekly and their demographic parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weekly grocery carts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table with demographic data and number of weekly purchases for each produce label to get an idea of their average grocery shopping carts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cart_df = trns.create_weekly_cart_df(trans_clean, participation_per_hh)\n",
    "\n",
    "#Drop the column of unfound labels as it's just missing information\n",
    "weekly_cart_df = weekly_cart_df.drop('not found_QUANT', axis=1)\n",
    "\n",
    "#Save it to csv:\n",
    "if not os.path.exists(\"saved_structures\"):\n",
    "    os.makedirs(\"saved_structures\")\n",
    "\n",
    "weekly_cart_df.to_csv(\"saved_structures/weekly_cart_df.csv\",\n",
    "                      sep='\\t',\n",
    "                      index=False)\n",
    "\n",
    "weekly_cart_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of labels which have very low values of weekly buying, so we drop them for the rest of the analysis. Thus, we are going to keep only the 8 labels that were bought the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels = trans_clean.groupby(\"LABEL\").count().sort_values(\n",
    "    by=\"PRODUCT_ID\", ascending=False).iloc[:8].index.tolist()\n",
    "\n",
    "#filtered_labels:\n",
    "trans_clean_filtered_labels = trans_clean.loc[trans_clean[\"LABEL\"].isin(\n",
    "    filtered_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the new filtered labels:\n",
    "plt_fx.cat_count_plot(\n",
    "    trans_clean_filtered_labels['LABEL'],\n",
    "    'Number of occurences per label in our filtered transaction data (only the most frequent labels in the most frequent departments)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels found most in weekly shopping carts at this retailer are: \n",
    "**UPDATE**\n",
    "- Processed foods\n",
    "- Produce\n",
    "- Dairy\n",
    "- Meat & Seafood\n",
    "- Beverages\n",
    "- Households\n",
    "- Condiments\n",
    "\n",
    "We create a new dataframe of weekly cart with filtered labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cart_df_filtered_labels = trns.create_weekly_cart_df(\n",
    "    trans_clean_filtered_labels, participation_per_hh)\n",
    "\n",
    "#Save it to csv:\n",
    "if not os.path.exists(\"saved_structures\"):\n",
    "    os.makedirs(\"saved_structures\")\n",
    "\n",
    "weekly_cart_df_filtered_labels.to_csv(\n",
    "    \"saved_structures/weekly_cart_df_filtered_labels.csv\",\n",
    "    sep='\\t',\n",
    "    index=False)\n",
    "\n",
    "weekly_cart_df_filtered_labels.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Weekly grocery cart and demographic information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the dataframe with demographic information and weekly shopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_trans_df = participation_hh_dem.join(weekly_cart_df_filtered_labels)\n",
    "dem_trans_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between transactions and demographic information: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look for shopping trends in our data, we want to calculate correlations between what households bought weekly and their demographic information. We also want to see if there is a correlation within the weekly cart itself (e.g. if buying more produce is related to buying more processed foods) and within demographic information. As our data is a mix of categorical and continuous variables, we need to calculate coefficients differently. For this we use [this](https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9) documentation to calculate a Cramer's V correlation and correlation ratio. Both correlation functions we used are in *transaction_fx.py*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create dictionary to indicate which columns are categorical or continuous: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_trans_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "list_of_col = [\n",
    "    'participation_length', 'mean weekly spending', 'mean yearly spending',\n",
    "    'AGE_DESC', 'MARITAL_STATUS_CODE', 'INCOME_DESC', 'HOMEOWNER_DESC',\n",
    "    'HOUSEHOLD_SIZE_DESC', 'KIDS_DESC', 'PRODUCE_QUANT',\n",
    "    'PROCESSED FOODS_QUANT', 'MEAT & SEAFOOD_QUANT', 'DAIRY_QUANT',\n",
    "    'BEVERAGES_QUANT', 'HOUSEHOLDS_QUANT', 'CONDIMENTS_QUANT'\n",
    "]\n",
    "\n",
    "cat_col = {\n",
    "    'participation_length': 0,\n",
    "    'mean weekly spending': 0,\n",
    "    'mean yearly spending': 0,\n",
    "    'AGE_DESC': 1,\n",
    "    'MARITAL_STATUS_CODE': 1,\n",
    "    'INCOME_DESC': 1,\n",
    "    'HOMEOWNER_DESC': 1,\n",
    "    'HOUSEHOLD_SIZE_DESC': 1,\n",
    "    'KIDS_DESC': 1,\n",
    "    'PRODUCE_QUANT': 2,\n",
    "    'PROCESSED FOODS_QUANT': 2,\n",
    "    'MEAT & SEAFOOD_QUANT': 2,\n",
    "    'DAIRY_QUANT': 2,\n",
    "    'BEVERAGES_QUANT': 2,\n",
    "    'HOUSEHOLDS_QUANT': 2,\n",
    "    'CONDIMENTS_QUANT': 2}\n",
    "\"\"\"\n",
    "\n",
    "list_of_col = [\n",
    "    'participation_length', 'mean weekly spending', 'mean yearly spending',\n",
    "       'AGE_DESC', 'MARITAL_STATUS_CODE', 'INCOME_DESC', 'HOMEOWNER_DESC',\n",
    "       'HOUSEHOLD_SIZE_DESC', 'KIDS_DESC', 'VEGETABLES_QUANT', 'FRUIT_QUANT',\n",
    "       'MEAT & SEAFOOD_QUANT', 'HOUSEHOLDS_QUANT',\n",
    "       'COOKIES SNACKS & CANDY _QUANT', 'CONDIMENTS/SPICES & BAKE_QUANT',\n",
    "       'DAIRY_QUANT', 'BEVERAGES_QUANT'\n",
    "]\n",
    "\n",
    "cat_col = {\n",
    "    'participation_length': 0,\n",
    "    'mean weekly spending': 0,\n",
    "    'mean yearly spending': 0,\n",
    "    'AGE_DESC': 1,\n",
    "    'MARITAL_STATUS_CODE': 1,\n",
    "    'INCOME_DESC': 1,\n",
    "    'HOMEOWNER_DESC': 1,\n",
    "    'HOUSEHOLD_SIZE_DESC': 1,\n",
    "    'KIDS_DESC': 1,\n",
    "    'VEGETABLES_QUANT':2, 'FRUIT_QUANT':2,\n",
    "       'MEAT & SEAFOOD_QUANT':2, 'HOUSEHOLDS_QUANT':2,\n",
    "       'COOKIES SNACKS & CANDY _QUANT':2, 'CONDIMENTS/SPICES & BAKE_QUANT':2,\n",
    "       'DAIRY_QUANT':2, 'BEVERAGES_QUANT':2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the correlation matrix, depending on the type of variables \n",
    "we use different correlation calculations:\n",
    "- categorical vs categorical : Cramer's V correlation \n",
    "- categorical vs continuous : correlation ratio\n",
    "- continuous vs continuous : Pearson correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_body = pd.DataFrame(index=list_of_col,\n",
    "                                columns=list_of_col,\n",
    "                                dtype=np.float64)\n",
    "for item in list_of_col:\n",
    "    for subitem in list_of_col:\n",
    "        if cat_col[item] == 1 and cat_col[subitem] == 1:\n",
    "            corr_matrix_body[item][subitem] = trns.cramers_v(\n",
    "                dem_trans_df[item], dem_trans_df[subitem])\n",
    "        elif (cat_col[item] == 1 and cat_col[subitem] == 0) or (cat_col[item] == 1 and cat_col[subitem] == 2):\n",
    "            corr_matrix_body[item][subitem] = trns.correlation_ratio(\n",
    "                dem_trans_df[item], dem_trans_df[subitem])\n",
    "        elif (cat_col[item] == 0 and cat_col[subitem] == 1) or (cat_col[item] == 2 and cat_col[subitem] == 1):\n",
    "            corr_matrix_body[item][subitem] = trns.correlation_ratio(\n",
    "                dem_trans_df[subitem], dem_trans_df[item])\n",
    "        elif cat_col[item] == 2 and cat_col[subitem] == 2:\n",
    "            corr_matrix_body[item][subitem] = stats.pearsonr(\n",
    "                weekly_cart_df[subitem], weekly_cart_df[item])[0]\n",
    "        else:\n",
    "            corr_matrix_body[item][subitem] = stats.pearsonr(\n",
    "                dem_trans_df[subitem], dem_trans_df[item])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation matrix as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(corr_matrix_body, annot=True)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                   rotation=45,\n",
    "                   horizontalalignment='right')\n",
    "\n",
    "#This is because the map was cut off on the top and bottom somehow:\n",
    "ax.set_ylim(len(corr_matrix_body), -0.5)\n",
    "ax.set(title='Correlation between shopping habits and demographic information')\n",
    "fig.savefig('correlation-matrix.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on graph:**\n",
    "\n",
    "The lighter the color, the higher the correlation between two instances. We notice several groups of interest in our data: \n",
    "\n",
    "- **Among demographic parameters**: \n",
    "    - The number of kids is highly correlated (*0.91*) with household size\n",
    "    - The marital status is highly correlated (*0.92*) with the household size  <br>\n",
    "   \n",
    "   \n",
    "   \n",
    "- **Among transaction parameters:** \n",
    "    - *Mean yearly spending* is highly correlated to *mean weekly spending* but this does not seem surprising\n",
    "    - There is a lighter group of correlation between weekly label quantities and mean weekly/yearly spending. The \"higher\" correlations (around *0.7*) are for the weekly quantities of produce, processed foods and household quantities. This implies that it is probable that a higher weekly spending is correlated with bigger quantities of those labels. It is a correlation lower than 0.8 (the normal threshold for significance) but our data is messy so it is still something interesting to explore. \n",
    "    - There is a slight correlation of 0.72 between *Meat & Seafood* and *Processed Foods*, it will be interesting to explore that. \n",
    "    \n",
    "    \n",
    "    \n",
    "- **Between demographic and transaction data:** there is no significant correlation between those categories. Correlation coefficients are extremely low and do not allow us to conclude any trend this way. \n",
    "\n",
    "We are going to pursue the demographic correlation a bit further to get an idea of the trend behind it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Correlation between number of kids, household size and marital status: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw 2 very high correlations among demographic data, we are going to look in more depth into it to understand the relationships between those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "sns.swarmplot(dem_trans_df[\"HOUSEHOLD_SIZE_DESC\"],\n",
    "              dem_trans_df[\"mean weekly spending\"],\n",
    "              hue=dem_trans_df[\"KIDS_DESC\"],\n",
    "              ax=ax[0])\n",
    "\n",
    "ax[0].set_title(\n",
    "    \"Mean weekly spending according to the number of kids and to the household size\"\n",
    ")\n",
    "\n",
    "sns.swarmplot(dem_trans_df[\"HOUSEHOLD_SIZE_DESC\"],\n",
    "              dem_trans_df[\"mean weekly spending\"],\n",
    "              hue=dem_trans_df[\"MARITAL_STATUS_CODE\"],\n",
    "              ax=ax[1])\n",
    "#ax[0,1] = fig2\n",
    "ax[1].set_title(\n",
    "    \"Mean weekly spending according to the marital status and to the household size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "On the figure above, we plotted the mean weekly spending against the household size, and colored the points according to the number of kids in the first sub-figure, and to the marital status code for the second sub-figure. \n",
    "We did this to have a clearer view of how the categorical data is correlated. Quite intuitively, a higher household size is correlated to a bigger number of kids, and to a married marital status. So, we learn nothing new about household size and number of kids except that we never have more than two adults living in a household. In our data we have quite traditional atomic households. This is further accentuated by the fact that almost all households with kids are married. \n",
    "\n",
    "We could not find a correlation between transactions and demographic information. We think it might be explained by the fact that we don't have that many data points to look at for demographic parameters, we further have no regular buying trends. Across all categories, households tend to shop at different intensities at this retailer, so it is hard to relate what they buy to their demographic information, knowing that they might buy additional products somewhere else. For example, if we wished to see whether people with higher income tended to buy more fresh products, we are missing information about their shopping as even though they bought small quantities here, they might have bought a lot on the market or somewhere else. \n",
    "\n",
    "Nevertheless, we still found some correlation between products which might be interesting to pursue (milestone 3). And because we looked at individual correlation between parameters and because it might be that a combination of different product quantities gives information about demographic parameters, we are going to look if a simple machine learning model finds something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision trees and random forests: \n",
    "As it seems that we cannot find a trend based on one type of product bought weekly, we need to take into account the combination of weekly shopping of different labels. For this wee need to use some machine learning. We are going to try and fit a decision tree. We fit a decision tree on a 0.25/0.75 test/training split as we start with not a lot of data in the beginning. For this we use the following documentation:\n",
    "- [Implementation and explanation of random forest:](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)\n",
    "- [Visualizing a decision tree:](https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c)\n",
    "- [Notebook with random forest/decision tree:](https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/Random%20Forest%20Tutorial.ipynb)\n",
    "- [Calculating multiclass AUC score:](https://medium.com/@plog397/auc-roc-curve-scoring-function-for-multi-class-classification-9822871a6659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision tree: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    'participation_length', 'mean weekly spending', 'mean yearly spending',\n",
    "    'VEGETABLES_QUANT', 'FRUIT_QUANT', 'MEAT & SEAFOOD_QUANT',\n",
    "    'HOUSEHOLDS_QUANT', 'COOKIES SNACKS & CANDY _QUANT',\n",
    "    'CONDIMENTS/SPICES & BAKE_QUANT', 'DAIRY_QUANT', 'BEVERAGES_QUANT'\n",
    "]\n",
    "\n",
    "dem_targets = [\n",
    "    'AGE_DESC', 'MARITAL_STATUS_CODE', 'INCOME_DESC', 'HOMEOWNER_DESC',\n",
    "    'HOUSEHOLD_SIZE_DESC', 'KIDS_DESC'\n",
    "]\n",
    "\n",
    "for target in dem_targets:\n",
    "    print('Decision tree on: ' + target)\n",
    "    tree = ml_fx.decision_tree(target, features, dem_trans_df)\n",
    "    print('____________________________________')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "Overall the decision trees are pretty bad. Their ROC AUC score is hardly above the baseline score (where the baseline score is the score we would obtain by just randomly assigning a label). Their accuracy is even worse. We would perform better than the model if we just guessed a label. We are going to look if a random forest performs better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in dem_targets:\n",
    "    print('Random forest on: ' + target)\n",
    "    tree = ml_fx.random_forest(target, features, dem_trans_df)\n",
    "    print('____________________________________')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "Though the random forest performs slightly better, the AUC score is still just slightly above 0.5. So we still might just guess randomly. \n",
    "\n",
    "Like explained for the correlations, we think this prediction model performs badly because it is trained on very few data points (the whole 2500 households would maybe have been better). Maybe performing PCA before training would make the results slightly better but we don't think the change would be significant. We also think that training a more complex model like a neural network would perform significantly better. A decision tree/random forest should be able to find trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next for milestone 3: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next milestone, we are going to look at the following to try and improve our results:\n",
    "\n",
    "- We focused until now on understanding the relationship between the demographic data and the transactions of the consumers. Thus, we had to filter out a lot of data because demographic data was only available for approximately one third of the households. Now, we saw that there are some correlations between product quantities, and we would like to know if they are accentuated in the whole transaction data.\n",
    "- We would also like to see if there are some kind of normalizations which would make sense. For example, normalize the product quantities to the number of people in the household\n",
    "- Analyse a boxplot of correlation values (for all 17 labels) per demographic category to look at the correlation distributions.\n",
    "\n",
    "To try to look at the data from a different point of view, we thought at the following points:\n",
    "\n",
    "- We created classes and labels for the type of products which are present in the transaction data. Now, we would like to know if we can find some correlations between demographic data and price classes for products. Especially, we would expect to see a correlation between price classes and income. \n",
    "- We could also try to look at the proportions. For example, we expect that the proportion of labels bought for each income class will be different.\n",
    "- We would like to try to improve our prediction model using PCA. Otherwise, we need to pursue another direction, because right now it does not seem possible to predict anything from shopping quantities. You may recall that we didn't use all the tables of the dataset, so we could imagine for example looking into the influence of the marketing (with the coupons) on shopping quantities.\n",
    "\n",
    "With those objectives, find below a sketch of our planning for the next project milestone:\n",
    "- Until **02.12** : try to improve our analysis\n",
    "- Until **09.12** : if the analysis didn't work, try another point of view, or go in depth in our analysis if it is worth it\n",
    "- Until **20.12** : clean the code, and write the report for milestone 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Household carts clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participation_per_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_trans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cart_df.sort_values(by=\"household_key\", inplace=True)\n",
    "weekly_cart_np = weekly_cart_df.to_numpy()[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(weekly_cart_np)\n",
    "weekly_cart_np_pca = pca.transform(weekly_cart_np)\n",
    "\n",
    "print(weekly_cart_np_pca.shape)\n",
    "plt.scatter(weekly_cart_np_pca[:,0], weekly_cart_np_pca[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, algorithm=\"elkan\").fit(weekly_cart_np)\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(weekly_cart_np_pca[:,0], weekly_cart_np_pca[:,1], c=kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_key = [i for i in range(len(kmeans.labels_))]\n",
    "\n",
    "hh_to_clust = pd.DataFrame(np.array([hh_key, kmeans.labels_]).T, index=None, columns=[\"household_key\", \"clust\"], dtype=int)\n",
    "\n",
    "hh_demographic_clust =\\\n",
    "    hh_demographic_fxd.join(hh_to_clust, on=\"household_key\", lsuffix=\"_clust\").drop(\"household_key_clust\", axis=1)\n",
    "hh_demographic_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weekly_dep_df(trans_clean, participation_per_hh):\n",
    "    \n",
    "    grouped_per_dep = pd.DataFrame(trans_clean.groupby(['DEPARTMENT','household_key']).sum())\n",
    "    index = trans_clean['household_key'].sort_values().unique()\n",
    "\n",
    "    weekly_dep_df = pd.DataFrame(index = index)\n",
    "    weekly_dep_df.index.name = 'household_key'\n",
    "\n",
    "    for dep in trans_clean['DEPARTMENT'].unique(): \n",
    "        data = [grouped_per_dep.loc[dep, i]['QUANTITY']/(participation_per_hh['participation_length'][i])\\\n",
    "                for i in grouped_per_dep.loc[dep].index]\n",
    "        \n",
    "        intermediary_df = pd.DataFrame(index = grouped_per_dep.loc[dep].index, data = {dep +'_QUANT': data})\n",
    "\n",
    "        weekly_dep_df = weekly_dep_df.join(intermediary_df)\n",
    "\n",
    "    #Fill NaN values with 0.0:\n",
    "    weekly_dep_df = weekly_dep_df.fillna(0.0)\n",
    "    \n",
    "    return weekly_dep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_dep_df = create_weekly_dep_df(trans_clean, participation_per_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_dep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_dep_np = weekly_dep_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(weekly_dep_np)\n",
    "weekly_dep_np_pca = pca.transform(weekly_dep_np)\n",
    "\n",
    "print(weekly_dep_np_pca.shape)\n",
    "plt.scatter(weekly_dep_np_pca[:,0], weekly_dep_np_pca[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering according to exact products: \n",
    "Look for total quantities of every unique product normalized to the number of weeks and select the products that were bought at least a certain amount of times (e.g. at least half of the weeks or more) so that we can look for typical weekly carts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group on the household key and product id:\n",
    "#We remove the columns that we are not interested in and add participation length\n",
    "grouped_house = trans_clean.groupby(['household_key', 'PRODUCT_ID']).sum()\n",
    "\n",
    "grouped_house = grouped_house.join(\n",
    "    participation_per_hh, on='household_key').drop(\n",
    "        ['BASKET_ID', 'DAY', 'STORE_ID', 'TRANS_TIME', 'WEEK_NO'], axis=1)\n",
    "\n",
    "grouped_house = grouped_house.reset_index(level='PRODUCT_ID')\n",
    "\n",
    "#Normalize the quantity to the number of participation weeks:\n",
    "grouped_house['QUANTITY'] = grouped_house['QUANTITY'] / grouped_house[\n",
    "    'participation_length']\n",
    "grouped_house.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the products that were bought at least 2/3 of the times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = 2 / 3\n",
    "weekly_products = grouped_house[grouped_house.QUANTITY.apply(\n",
    "    lambda x: x >= boundary)]\n",
    "#Add product information to the quantities:\n",
    "weekly_products = weekly_products.join(labelled_prod, on='PRODUCT_ID')\n",
    "\n",
    "#How many products are left ? i\n",
    "print(\"There are now : %d products bought regurarly weekly by the households\" %\n",
    "      len(weekly_products.PRODUCT_ID.unique()))\n",
    "\n",
    "weekly_products.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save it to csv: \n",
    "weekly_products_df  = weekly_products.copy()\n",
    "weekly_products_df['household_key'] = weekly_products.index\n",
    "\n",
    "if not os.path.exists(\"saved_structures\"):\n",
    "    os.makedirs(\"saved_structures\")\n",
    "weekly_products_df.to_csv(\"saved_structures/weekly_products.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now transform the dataframe to be able to do clustering. For this we create a very sparse dataframe with a column for each quantity bought weekly for product: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantities = weekly_products.groupby(['household_key', 'PRODUCT_ID']).sum()\n",
    "quantities = quantities.drop(['participation_length', 'SALES_VALUE'],\n",
    "                             axis=1).unstack(level=-1, fill_value=0.0)\n",
    "\n",
    "quantities.columns = quantities.columns.droplevel()\n",
    "quantities.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save it to csv:\n",
    "\n",
    "#Add the index as a column to save it:\n",
    "quantities_df = quantities.copy()\n",
    "quantities_df['household_key'] = quantities_df.index\n",
    "\n",
    "if not os.path.exists(\"saved_structures\"):\n",
    "    os.makedirs(\"saved_structures\")\n",
    "quantities_df.to_csv(\"saved_structures/big_table_quantities.csv\",\n",
    "                     sep='\\t',\n",
    "                     index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a very sparse and large matrix (1638 columns). Before doing any clustering, we need to apply dimensionality reduction before doing any clustering. We choose to apply SVD before as dimension reduction as  SVD is more appropriate compared to PCA for sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, n_iter=7, random_state=42)\n",
    "#Get a new matrix that is not sparse anymore:\n",
    "quantities_transformed = svd.fit_transform(quantities)\n",
    "\n",
    "#Print how much variance the features explain in total:\n",
    "print(\"Total variance explained by the svd features:\" +\n",
    "      str(svd.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check and project it on 2d space, we make a 2 feature svd to have a look at potential clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd2comp = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "quantities_transformed_2comp = svd2comp.fit_transform(quantities)\n",
    "\n",
    "print(\"Total variance explained by the 2 feature svd : \" +\n",
    "      str(svd2comp.explained_variance_ratio_.sum()))\n",
    "\n",
    "df = pd.DataFrame(quantities_transformed_2comp)\n",
    "plt.scatter(x=df[0], y=df[1])\n",
    "plt.title('Two component SVD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be two directions but the total variance explained by two features is only 6.5% so there is no way this image is representative of our data. We need to take into account more features. For this we go back to our 300 features explaining around 87% of total variance and try to find clusters. We choose to apply K-means and first try to find the ideal number of clusters with the elbow method. We try a number of clusters between 4 and 20. This clusters would ideally indicate people that have approximately the same shopping habits (i.e who buy the same products). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#Look at some ideal cluster sizes:\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(4, 20), metric='calinski_harabasz')\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "visualizer.fit(quantities_transformed)\n",
    "\n",
    "# Finalize and render the figure\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best cluster number seems to be between 5-9. We will try to look at 7 clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to find clusters with k-means:\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(quantities_transformed)\n",
    "clustered_households = pd.DataFrame(data={\n",
    "    'household_key': quantities.index,\n",
    "    'labels': kmeans.labels_\n",
    "})\n",
    "\n",
    "sns.countplot(clustered_households.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have one huge cluster and some small ones. To understand which households were clustered together and on what basis, we will take a closer look at their shopping. To decide that households bought the same, we set the threshold that for a product to be characteristic of a cluster, it needs to have been bought by at least a third of the households (we set the bar really low...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    ml_fx.observe_clusers(i,clustered_households, labelled_prod, quantities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "It looks like we have some peculiar clusters. Somehow we have clusters with only one household and another cluster that has no products in common. It looks like this data is just too precise and sparse to conclude anything, except that apparently the only clusters it finds is households that buy bananas and milk. As there are several types of products that are the same (e.g. just a different brand or product id), it's hard for the algorithm to find similar shopping habits as those products will considered different while being the same. We need to take a step back and look at the global level again, i.e go back to departments and labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go back to labels and departments: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cart_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "#Get a new matrix that is not sparse anymore:\n",
    "labels_transformed = svd.fit_transform(weekly_cart_df)\n",
    "\n",
    "#Print how much variance the features explain in total:\n",
    "print(\"Total variance explained by the svd features:\" +\n",
    "      str(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_with_dem(df_dem):\n",
    "    demogrpahics = [\n",
    "        'AGE_DESC', 'MARITAL_STATUS_CODE', 'INCOME_DESC', 'HOMEOWNER_DESC',\n",
    "        'HOUSEHOLD_SIZE_DESC', 'KIDS_DESC'\n",
    "    ]\n",
    "    \n",
    "    for dem in demogrpahics:\n",
    "        fig = plt.figure()\n",
    "        sns.scatterplot(x=df_dem[0], y=df_dem[1], data=df_dem, hue=dem)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a demographic feature:\n",
    "df_dem = pd.DataFrame(data=labels_transformed, index=weekly_cart_df.index)\n",
    "df_dem = df_dem.join(dem_trans_df).dropna()\n",
    "\n",
    "scatter_with_dem(df_dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most households seem to be clustered in the same cluster. The 2d approximation is not even that bad as we explain around 70% of total variance... So these labels are just to global to conclude anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can still try to find some clusters with K-means:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "\n",
    "#Get a new matrix that is not sparse anymore:\n",
    "labels_transformed = svd.fit_transform(weekly_cart_df)\n",
    "\n",
    "#Print how much variance the features explain in total:\n",
    "print(\"Total variance explained by the svd features:\" +\n",
    "      str(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2, 20), metric='calinski_harabasz')\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "visualizer.fit(labels_transformed)\n",
    "\n",
    "# Finalize and render the figure\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad clustering, does not find any knee or elbow point. No actual clusters are being formed. Try clustering without any dimensionality reduction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2, 20), metric='calinski_harabasz')\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "visualizer.fit(weekly_cart_df)\n",
    "\n",
    "# Finalize and render the figure\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same problem, absolutely no clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Departments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "\n",
    "#Get a new matrix that is not sparse anymore:\n",
    "departments_transformed = svd.fit_transform(weekly_dep_df)\n",
    "\n",
    "#Print how much variance the features explain in total:\n",
    "print(\"Total variance explained by the svd features:\" +\n",
    "      str(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a demographic feature:\n",
    "df_dem_dep = pd.DataFrame(data=departments_transformed, index=weekly_cart_df.index)\n",
    "df_dem_dep = df_dem_dep.join(hh_demographic_fxd).drop('household_key', axis = 1).dropna()\n",
    "df_dem_dep.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_with_dem(df_dem_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same comment as for labels, still seems to global. Try finding clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=3, n_iter=7, random_state=42)\n",
    "\n",
    "#Get a new matrix that is not sparse anymore:\n",
    "dep_transformed = svd.fit_transform(weekly_dep_df)\n",
    "\n",
    "#Print how much variance the features explain in total:\n",
    "print(\"Total variance explained by the svd features:\" +\n",
    "      str(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2, 20), metric='calinski_harabasz')\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "visualizer.fit(dep_transformed)\n",
    "\n",
    "# Finalize and render the figure\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to find an ideal cluster number of 4. Let's look at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(dep_transformed)\n",
    "\n",
    "clustered_households = pd.DataFrame(data={\n",
    "    'household_key': weekly_dep_df.index,\n",
    "    'labels': kmeans.labels_\n",
    "})\n",
    "\n",
    "sns.countplot(clustered_households.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution accross departments does not look that bad. Let's have a closer look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame(index = range(4),columns = weekly_dep_df.columns)\n",
    "for i in range(4):\n",
    "    median = ml_fx.observe_clusers_departments(i, clustered_households, labelled_prod, weekly_dep_df, give_clusters = True)\n",
    "    df_clusters.loc[i] = median\n",
    "df_clusters = df_clusters.transpose()\n",
    "df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'yellow']\n",
    "\n",
    "for i in range(4):\n",
    "    axs = sns.stripplot(x=df_clusters.index, y=df_clusters[i], color=colors[i])\n",
    "\n",
    "axs.set_xticklabels(axs.get_xticklabels(),\n",
    "                    rotation=45,\n",
    "                    horizontalalignment='right')\n",
    "axs.set(xlabel='Departments',\n",
    "        ylabel='Weekly quantities',\n",
    "        title='Median weekly quantities per cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pain in the butt to add labels, but anyway we see that it mostly clustered according to the quantity bought in the grocery department. That's quite interesting. So we do have four types of shopping habits in groceries quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dem = clustered_households.join(dem_trans_df, lsuffix = '_').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_col = ['participation_length',\n",
    "       'mean weekly spending', 'mean yearly spending', 'AGE_DESC',\n",
    "       'MARITAL_STATUS_CODE', 'INCOME_DESC', 'HOMEOWNER_DESC',\n",
    "       'HOUSEHOLD_SIZE_DESC', 'KIDS_DESC', 'VEGETABLES_QUANT', 'FRUIT_QUANT',\n",
    "       'MEAT & SEAFOOD_QUANT', 'HOUSEHOLDS_QUANT',\n",
    "       'COOKIES SNACKS & CANDY _QUANT', 'CONDIMENTS/SPICES & BAKE_QUANT',\n",
    "       'DAIRY_QUANT', 'BEVERAGES_QUANT'\n",
    "]\n",
    "\n",
    "cat_col = {\n",
    "    'participation_length': 0,\n",
    "    'mean weekly spending': 0,\n",
    "    'mean yearly spending': 0,\n",
    "    'AGE_DESC': 1,\n",
    "    'MARITAL_STATUS_CODE': 1,\n",
    "    'INCOME_DESC': 1,\n",
    "    'HOMEOWNER_DESC': 1,\n",
    "    'HOUSEHOLD_SIZE_DESC': 1,\n",
    "    'KIDS_DESC': 1,\n",
    "    'VEGETABLES_QUANT':2, 'FRUIT_QUANT':2,\n",
    "       'MEAT & SEAFOOD_QUANT':2, 'HOUSEHOLDS_QUANT':2,\n",
    "       'COOKIES SNACKS & CANDY _QUANT':2, 'CONDIMENTS/SPICES & BAKE_QUANT':2,\n",
    "       'DAIRY_QUANT':2, 'BEVERAGES_QUANT':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrlation_matrix_with_dem(dem_trans_df, list_of_col, cat_col):\n",
    "    corr_matrix_body = pd.DataFrame(index=list_of_col,\n",
    "                                columns=list_of_col,\n",
    "                                dtype=np.float64)\n",
    "    for item in list_of_col:\n",
    "        for subitem in list_of_col:\n",
    "            if cat_col[item] == 1 and cat_col[subitem] == 1:\n",
    "                corr_matrix_body[item][subitem] = trns.cramers_v(\n",
    "                    dem_trans_df[item], dem_trans_df[subitem])\n",
    "            elif (cat_col[item] == 1 and cat_col[subitem] == 0) or (cat_col[item] == 1 and cat_col[subitem] == 2):\n",
    "                corr_matrix_body[item][subitem] = trns.correlation_ratio(\n",
    "                    dem_trans_df[item], dem_trans_df[subitem])\n",
    "            elif (cat_col[item] == 0 and cat_col[subitem] == 1) or (cat_col[item] == 2 and cat_col[subitem] == 1):\n",
    "                corr_matrix_body[item][subitem] = trns.correlation_ratio(\n",
    "                    dem_trans_df[subitem], dem_trans_df[item])\n",
    "            elif cat_col[item] == 2 and cat_col[subitem] == 2:\n",
    "                corr_matrix_body[item][subitem] = stats.pearsonr(\n",
    "                    weekly_cart_df[subitem], weekly_cart_df[item])[0]\n",
    "            else:\n",
    "                corr_matrix_body[item][subitem] = stats.pearsonr(\n",
    "                    dem_trans_df[subitem], dem_trans_df[item])[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = sns.heatmap(corr_matrix_body, annot=True)\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                       rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "\n",
    "    #This is because the map was cut off on the top and bottom somehow:\n",
    "    ax.set_ylim(len(corr_matrix_body), -0.5)\n",
    "    ax.set(title='Correlation between shopping habits and demographic information')\n",
    "    fig.savefig('correlation-matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at biggest cluster: \n",
    "cluster_4 = clusters_dem[clusters_dem.labels == 3]\n",
    "corrlation_matrix_with_dem(cluster_4, list_of_col, cat_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have some bigger correlation now but I'm not sure if we can conclude anything from that. But for this cluster it looks like we have the following correlations: \n",
    "- household size vs quantity of dairy \n",
    "- quantity of dairy vs number of kids\n",
    "- number of kids and household size vs processed foods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No correlation jumping to the eyes for the other clusters but you're welcome to look at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = clusters_dem[clusters_dem.labels == 0]\n",
    "cluster_2 = clusters_dem[clusters_dem.labels == 1]\n",
    "cluster_3 = clusters_dem[clusters_dem.labels == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do they have anything in common in terms of labels ?\n",
    "#Cluster 4:\n",
    "\n",
    "plt_fx.pie_plot_labels(cluster_1)\n",
    "plt_fx.pie_plot_labels(cluster_2)\n",
    "plt_fx.pie_plot_labels(cluster_3)\n",
    "plt_fx.pie_plot_labels(cluster_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing jumps out regarding proportions of labels, the proportions look the same. All groups spend more on processed foods and produce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall label proportions with demographics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = dem_trans_df.AGE_DESC.unique()\n",
    "incomes = dem_trans_df.INCOME_DESC.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for income in incomes:\n",
    "    print(income)\n",
    "    plt_fx.pie_plot_labels(dem_trans_df[dem_trans_df.INCOME_DESC == income])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "440px",
    "left": "330px",
    "top": "220px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
